# Exploration of Learner Data with VAEs
Given student test scores, can a machine understand with granularity what a student knows about a given subject?  If a student scores an 65 on a trig test, does the student not understand the trigonometry, or just the algebra behind the trig?  "Autoencoders for Educational Assessment" by Converse, et al., found a way to automate the process of finding latent student knowledge using Variational Autoencoders.  Through integrating the variational autoencoder with some outside information, the authors were able to obtrain strong estimates for ground truth student knowledge.  Moreover, they were able to uncover exactly how the tests themselves were formed were great precision.  

This github repository is devoted to reproducing the findings of "Autoencoders for Educational Assessment" and extending the research of the paper.  The first part of this will focus on my replication.  Following this, I will present several questions that I have answered concerning the architecture and data consumptions of the VAEs.  After this, I will present future areas for the next teams to tackle.    

## Replication of the Study
Using Tensorflow, I was able to recreate the paper.  The general results were the same, though some of the specifics had changed.  This is not surprising, as the data are generated by stochastic processes.  
The first table of the paper illustrates the difference in parameter recovery between variational autoencoders (VAEs) and autoencoders (AEs).  a_1,a_2,a3,b are all parameters used to generated test questions.  

<img src="Images/Table_1.PNG" height = 200>

Clearly, a variational autoencoder recovers the parameters of the test better than an autoencoder.  This can be seen visually
from a plot of the recreations of the VAE and AE of the test data.

<img src="Images/Parameter%20Recovery.png" height = 200>

It is clear that the VAE much more clearly correlates with the unobserved ground truth used to create the test.  Even more striking is the recovery of the difficulty parameter b.  As seen below, the VAE correlates almost perfectly with the ground truth of question difficulty.

<img src="Images/fig4.png" height = 200>


The second table shows a VAE and an AE recovering the students' knowledge of a subject.

<img src="Images/Table_2_2.PNG" height = 200>

While the VAE has less error in terms of AVRB and RMSE, the AE correlates between with the student knowledge.  Below I have graphed the student knowledge compared to the ground truth.

<img src="Images/fig5.png" height = 200>

## Extensions 
The paper left room for many additional questions to be asked about the capabilities of VAEs for student assessment.  I looked at the following:
1. Which architectures can best extract the learner data? 
    1. How much dropout is necessary?
    1. Which activations work best?
1. How do VAEs perform with different amounts of learner data?
1. Do some hidden distributions work better than others?

I have tentative answers for the first question.  
## Future Work


The author has 1) replicated the findings of “Autoencoders for Educational Assessment,” 2) created a function to test different a) data and b) architectural assumptions for the VAE than those provided by the paper, 3) written additional hidden distribution to test, and 4) written a multilevel vae for testing subject where knowledge of one subject affects knowledge of another.  

Introduction
Autoencoders for Educational Assessment showed the viability of using VAEs to uncover the student understanding in item response theory models.  The hidden knowledge of students highly correlated to the true hidden knowledge that was simulated.  Additionally, the VAE was able to estimate facts about the composition of the test itself with high correlation to the ground truth.  
In the experiments given by the paper, the architecture of the VAE was as follows:
1.	 28 dim input
2.	10 dim hidden layer with sigmoid activation
3.	3 dim normal stochastic layer outputting 
a.	Statics of the stochastic function
b.	The stochastic output
4.	28 dim output layer with sigmoid activation
a.	Q-matrix is multiplied by the weight matrix to give interpretability.

The input and output layer both had 28 layers because the tests fed into the network had 28 questions.  The stochastic layer had 3 dimensions because the assessment tested 3 hidden knowledge traits.  The paper gave room for several extensions.  The author chose to focus on the following:
1)	How does model architecture affect quality?
2)	How much data is necessary for the VAE uncover both test and student data with high correlation to ground truth?  
3)	Are the predictions invariant to the distribution of the activation function?  
a.	Double exponential vs Normal
b.	Quantiles of gamma vs Normal 
i.	Bin the gammas into quantiles and match correlation with normal quantiles
4)	What happens if the student knowledge is nested?
a.	Some hidden knowledge creates others, i.e. Using algebra to solve a trig problem	
b.	 Some components have covariance. i.e. reading skill affects writing skill
5)	A future direction would be what to do if only a partially defined Q matrix is available.  
a.	Semi supervised learning to determine rest of Q-matrix


Replication. 
 The author was able to replicate the paper.  While the RMSE and MAPE of the author were lower than the origenal paper, the correlations where worse.

 The aut  

